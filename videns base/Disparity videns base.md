Loop, C. and Zhang, Z. 1999. Computing rectifying homographies for stereo vision. In CVPR, Vol. I, pp. 125–131

- this paper shows the importance of having a setup that is as close to parallel as possible
- while its possible to recitfy images after the fact, the article show illtuatres how this produces noise, loss of information and other artifacts
- Afsnit 5.1 fortæller om distortion minimization criterion



# Disparity map generation a timeline

### Žbontar, J. and LeCun, Y. (2015) ‘Computing the stereo matching cost with a convolutional neural network’, in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.
MC-CNN (Multi Column Convolutional Neural Network)

- Deep features are extracted from 9x9 pixel patches in the left and right images, using a kernel of 5x5. For each disparity level, each pixel in the left image is compared to the pixel in the right image at the same location plus the disparity level. The comparison is done using a convolutional neural network, which is trained to minimize the difference between the left and right patches. The result is a cost volume, which is then used to generate the disparity map. The cost volume is generated by stacking the cost for each disparity level in the third dimension. At each entry in a dimension, there is a vector of shape 2x1, which is a probability distribution over the negative and positive match.

- Given the cost volume, the cross-based cost aggregation is used. The cross-based cost aggregation is an algorithm which considers windows or patches of morphing size around each pixel in the cost volume. Given a window in a disparity level, 

**Great Quotes**

> *Stereo matching, which aims to estimate depth (or dis parity) from a pair of rectified stereo images, is a fundamental task for many robotics and computational photography applications, such as 3D reconstruction, robot navigation and autonomous driving.*

### Kendall, A. et al. (2017) ‘End-to-End Learning of Geometry and Context for Deep Stereo Regression’, Proceedings of the IEEE International Conference on Computer Vision, 2017-Octob, pp. 66–75.
GC-Net (Geometry and Context Network)

- For instance, GC-Net[10] proposes a 3D encoder-decoder structure to reason about global geometry of the scene from the 4D cost volume (Xu et al., 2023).

- GC-Net[10] constructs a 4D cost volume by concatenating the left and right CNN features and then utilizes a 3D encoder-decoder network to aggregate and regularize the cost volume (Xu et al., 2023).

### Chang, J.R. and Chen, Y.S. (2018) ‘Pyramid Stereo Matching Network’, in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Available at: https://doi.org/10.1109/CVPR.2018.00567.
PSM-Net (Pyramid Stereo Matching Network)

- PSMNet[2] and GwcNet[31] exploit the stacked 3D encoder-decoder structure to regularize cost volume, achieving great improvement in accuracy (Xu et al., 2023).


### Guo, X. et al. (2019) ‘Group-wise correlation stereo network’, in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Available at: https://doi.org/10.1109/CVPR.2019.00339.
GWC-Net (Group-wise Correlation Stereo Network)

- PSMNet[2] and GwcNet[31] exploit the stacked 3D encoder-decoder structure to regularize cost volume, achieving great improvement in accuracy (Xu et al., 2023).
- 

### Zhang, F. et al. (2019) ‘GA-net: Guided aggregation net for end-to-end stereo matching’, in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Available at: https://doi.org/10.1109/CVPR.2019.00027.
GaNet

- To improve efficiency of cost aggregation, GANet[38] proposes two guided aggregation layers to replace abundant 3D convolutions (Xu et al., 2023)

### Xu, G. et al. (2022) ‘Attention Concatenation Volume for Accurate and Efficient Stereo Matching’, in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Available at: https://doi.org/10.1109/CVPR52688.2022.01264.
ACV-Net (Attention Concatenation Volume Network)

- 


### Xu, G., Zhou, H. and Yang, X. (2023) ‘CGI-Stereo: Accurate and Real-Time Stereo Matching via Context and Geometry Interaction’. Available at: http://arxiv.org/abs/2301.02789.
CGI-Stereo (Context and Geometry Interaction Stereo Network)

architecture

- multi-scale feature extraction
- attention feature volume construction
- cost aggregation
- disparity prediction

- Use pretrained MobileNetV2
- contribution 
    - Context and Geometry Fusion
    - new cost volume; Attention Feature Volume
    - accurate and real-time stereo matching network (why is it real-time?)



<!-- ---------------------------------------------------------------------- -->
# Datasets

### A., Lenz, P. and Urtasun, R. (2012) ‘Are we ready for autonomous driving? the KITTI vision benchmark suite’, in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Available at: https://doi.org/10.1109/CVPR.2012.6248074.
KITTI dataset

-


### Jensen, R. et al. (2014) ‘Large scale multi-view stereopsis evaluation’, in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Available at: https://doi.org/10.1109/CVPR.2014.59.
DTU dataset

-


### Mayer, N. et al. (2016) ‘A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation’, in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Available at: https://doi.org/10.1109/CVPR.2016.438.
SceneFlow dataset

-


### Scharstein, D. et al. (2014) ‘High-resolution stereo datasets with subpixel-accurate ground truth’, in Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). Available at: https://doi.org/10.1007/978-3-319-11752-2_3.
Middlebury dataset

-


### Schöps, T. et al. (2017) ‘A multi-view stereo benchmark with high-resolution images and multi-camera videos’, in Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017. Available at: https://doi.org/10.1109/CVPR.2017.272.
ETH3D dataset

-

